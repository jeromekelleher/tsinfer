
import random
import os
import h5py
import zarr
import sys
import pandas as pd
import daiquiri
#import bsddb3
import time
import scipy
import pickle
import collections
import itertools
import tqdm
import shutil
import pprint
import numpy as np
import json

import matplotlib as mp
# Force matplotlib to not use any Xwindows backend.
mp.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns

import tsinfer
import msprime



def plot_breakpoints(ts, map_file, output_file):
    # Read in the recombination map using the read_hapmap engine,
    recomb_map = msprime.RecombinationMap.read_hapmap(map_file)

    # Now we get the positions and rates from the recombination
    # map and plot these using 500 bins.
    positions = np.array(recomb_map.get_positions()[1:])
    rates = np.array(recomb_map.get_rates()[1:])
    num_bins = 500
    v, bin_edges, _ = scipy.stats.binned_statistic(
        positions, rates, bins=num_bins)
    x = bin_edges[:-1][np.logical_not(np.isnan(v))]
    y = v[np.logical_not(np.isnan(v))]
    fig, ax1 = plt.subplots(figsize=(16, 6))
    ax1.plot(x, y, color="blue", label="Recombination rate")
    ax1.set_ylabel("Recombination rate")
    ax1.set_xlabel("Chromosome position")

    # Now plot the density of breakpoints along the chromosome
    breakpoints = np.array(list(ts.breakpoints()))
    ax2 = ax1.twinx()
    v, bin_edges = np.histogram(breakpoints, num_bins, density=True)
    ax2.plot(bin_edges[:-1], v, color="green", label="Breakpoint density")
    ax2.set_ylabel("Breakpoint density")
    ax2.set_xlim(1.5e7, 5.3e7)
    plt.legend()
    fig.savefig(output_file)


def make_errors(v, p):
    """
    For each sample an error occurs with probability p. Errors are generated by
    sampling values from the stationary distribution, that is, if we have an
    allele frequency of f, a 1 is emitted with probability f and a
    0 with probability 1 - f. Thus, there is a possibility that an 'error'
    will in fact result in the same value.
    """
    w = np.copy(v)
    if p > 0:
        m = v.shape[0]
        frequency = np.sum(v) / m
        # Randomly choose samples with probability p
        samples = np.where(np.random.random(m) < p)[0]
        # Generate observations from the stationary distribution.
        errors = (np.random.random(samples.shape[0]) < frequency).astype(int)
        w[samples] = errors
    return w


def generate_samples(ts, error_p):
    """
    Returns samples with a bits flipped with a specified probability.

    Rejects any variants that result in a fixed column.
    """
    S = np.zeros((ts.sample_size, ts.num_mutations), dtype=np.int8)
    for variant in ts.variants():
        done = False
        # Reject any columns that have no 1s or no zeros
        while not done:
            S[:, variant.index] = make_errors(variant.genotypes, error_p)
            s = np.sum(S[:, variant.index])
            done = 0 < s < ts.sample_size
    return S.T


def tsinfer_dev(
        n, L, seed, num_threads=1, recombination_rate=1e-8,
        error_rate=0, engine="C", log_level="WARNING",
        debug=True, progress=False, path_compression=True):

    np.random.seed(seed)
    random.seed(seed)
    L_megabases = int(L * 10**6)

    # daiquiri.setup(level=log_level)

    ts = msprime.simulate(
            n, Ne=10**4, length=L_megabases,
            recombination_rate=recombination_rate, mutation_rate=1e-8,
            random_seed=seed,
            model="smc_prime")
    if debug:
        print("num_sites = ", ts.num_sites)
    assert ts.num_sites > 0

    # sample_data = tsinfer.SampleData.from_tree_sequence(ts, compressor=None)
    V = generate_samples(ts, error_rate)
    with tsinfer.SampleData(compressor=None) as sample_data:
        for site in ts.sites():
            sample_data.add_site(site.position, V[site.id])
    # print(sample_data)

    time_type = "frequency"
    ancestor_data = tsinfer.generate_ancestors(
        sample_data, engine=engine, num_threads=num_threads)

#     # True time doesn't work with error at the moment
#     time_type = "true"
#     time_map = collections.defaultdict(list)
#     j = 0
#     for tree in ts.trees():
#         for site in tree.sites():
#             assert len(site.mutations) == 1
#             node = site.mutations[0].node
#             if tree.num_samples(node) > 1:
#                 time_map[ts.node(node).time].append(j)
#                 j += 1
#     times = [None for _ in range(j)]
#     for j, time in enumerate(sorted(time_map.keys()), start=2):
#         # print(j, time, time_map[time])
#         for site in time_map[time]:
#             times[site] = j
#     assert len(times) == sample_data.num_inference_sites
#     # print(times)
#     ancestor_data = tsinfer.AncestorData(sample_data)
#     generator = tsinfer.AncestorsGenerator(
#         sample_data, ancestor_data, tsinfer.DummyProgressMonitor(),
#         engine=engine, num_threads=num_threads)
#     generator.add_sites(times)
#     generator.run()

    print(time_type, "time")
    # Generate the true ancestors from the ts.
    # ts = subset_sites(ts, ancestor_data.sites_position[:])
    ts = tsinfer.minimise(ts)
    tables = ts.dump_tables()

    site_map = {site.position: site.id for site in  ts.sites()}
    site_map[0.0] = 0
    site_map[ts.sequence_length] = len(site_map)
    tables.nodes.set_columns(
        flags=np.ones_like(tables.nodes.flags),
        time=tables.nodes.time)
    # print(tables)
    ts = tables.tree_sequence()
    H = ts.genotype_matrix().T.astype(int)
    start = {u: ts.sequence_length for u in range(ts.num_nodes)}
    end = {u: 0 for u in range(ts.num_nodes)}
    for e in ts.edges():
        start[e.parent] = min(start[e.parent], e.left)
        end[e.parent] = max(end[e.parent], e.right)

    for node in ts.nodes():
        if end[node.id] != 0:
            left = site_map[start[node.id]]
            right = site_map[end[node.id]]
            haplotype = H[node.id][:]
            haplotype[:left] = -1
            haplotype[right:] = -1
            # h = "".join(map(str, haplotype[left: right]))
            # print(node.id, left, right, h, sep="\t")
            assert left < right
            assert np.all(haplotype[left: right] >= 0)

    # Make a mapping from focal sites to ancestors.
    true_site_ancestor_map = {}
    for site in ts.sites():
        assert len(site.mutations) == 1
        node = site.mutations[0].node
        true_site_ancestor_map[site.id] = H[node][:]
        # print("site = ", site.id, "node = ", node)
        assert true_site_ancestor_map[site.id][site.id] == 1

    # The equivalent map for generated ancestors.
    inferred_site_ancestor_map = {}
    inference_sites = np.where(sample_data.sites_inference[:])[0]
    ancestor_site_position = ancestor_data.sites_position[:]
    M = sample_data.num_sites
    site_time = np.zeros(M, dtype=int)
    num_ancestor_sites = ancestor_data.num_sites
    # focal_site_time = np.zeros(ancestor_data.num_sites)
    total_sites = 0
    total_ones = 0
    for ancestor in ancestor_data.ancestors():
        # First map the ancestor haplotype into an array with the number of
        # inference sites.
        b = np.zeros(num_ancestor_sites, dtype=np.int8) - 1
        b[ancestor.start: ancestor.end] = ancestor.haplotype
        # Now map this into an array of the full size.
        a = np.zeros(M, dtype=np.int8)
        start = inference_sites[ancestor.start]
        if ancestor.end == num_ancestor_sites:
            end = M
        else:
            end = inference_sites[ancestor.end]
        a[:start] = -1
        a[end:] = -1
        a[inference_sites] = b
        total_sites += end - start
        total_ones += np.sum(a[start: end] == 1)
        for focal_site in ancestor.focal_sites:
            mapped_site = inference_sites[focal_site]
            assert start <= mapped_site < end
            assert mapped_site not in inferred_site_ancestor_map
            site_time[mapped_site] = ancestor.time
            inferred_site_ancestor_map[mapped_site] = a

    print("total_sites", total_sites)
    print("total_ones", total_ones)
    print("fraction", total_ones / total_sites)

    # Analyse the data.

    overlap_count = 0
    mismatch_count = 0
    # Arrays for the dataframe stratifying by time.
    overlaps = []
    mismatches = []
    times = []
    for site, inferred_ancestor in inferred_site_ancestor_map.items():
        time = site_time[site]
        true_ancestor = true_site_ancestor_map[site]
        assert inferred_ancestor[site] == 1
        assert true_ancestor[site] >= 0
        true_length = np.sum(true_ancestor >= 0)
        length = np.sum(inferred_ancestor >= 0)
        # ancestor_comparisons += 1
        # short_count += length < true_length
        overlap = np.where(np.logical_and(inferred_ancestor >= 0, true_ancestor >= 0))[0]
        assert len(overlap) > 0
        start = overlap[0]
        end = overlap[-1] + 1
        assert np.array_equal(np.arange(start, end, dtype=int), overlap)
        mismatch_index = inferred_ancestor[start: end] != true_ancestor[start: end]
        mismatch = np.sum(mismatch_index)
        overlap_count += end - start
        mismatch_count += mismatch
        times.append(time)
        mismatches.append(mismatch)
        overlaps.append(end - start)

        # print(site, "@t =", time)
        # print("\t", inferred_ancestor[start: end])
        # print("\t", true_ancestor[start: end])
        # print("\t", end - start, mismatch)


    print("Total mismatch rate = ", mismatch_count / overlap_count)
    data = pd.DataFrame({
        "overlap": overlaps, "mismatches": mismatches, "time": times})

    total = data.groupby(data.time).sum()
    assert np.sum(data.overlap) == overlap_count
    assert np.sum(data.mismatches) == mismatch_count

    plt.plot(total.mismatches / total.overlap, ".")
    plt.xlabel("Time")
    plt.ylabel("Error")
    plt.savefig("tmp__NOBACKUP__/error-{}.png".format(time_type))
    plt.clf()

    plt.plot(total.overlap, ".")
    plt.xlabel("Time")
    plt.ylabel("Base count")
    plt.savefig("tmp__NOBACKUP__/count-{}.png".format(time_type))
    plt.clf()

    # print(site_ancestor_map)
    # position = ancestor_data.sites_position[:]
    # # inference_sites = sample_data.sites_inference[:]
    # overlap_count = 0
    # mismatch_count = 0
    # short_count = 0
    # ancestor_comparisons = 0
    # older_site_count = 0
    # equal_site_count = 0
    # younger_site_count = 0
    # # Arrays for the dataframe stratifying by time.
    # overlaps = []
    # mismatches = []
    # times = []
    # # The first index here is zero if the error site is older than the
    # # focal site, 0 if it is equal, and 1 otherwise. The second index is the
    # # value of mismatching base.
    # mismatch_type_count = np.zeros((3, 2), dtype=int)
    # for ancestor in ancestor_data.ancestors():
    #     # print(ancestor)
    #     a = np.zeros(ancestor_data.num_sites, dtype=np.int8)
    #     a[ancestor.start: ancestor.end] = ancestor.haplotype
    #     for site in ancestor.focal_sites:
    #         # print("Site = ", position[site], ancestor.time)
    #         true_ancestor = site_ancestor_map[position[site]]
    #         true_length = np.sum(true_ancestor >= 0)
    #         length = np.sum(a >= 0)
    #         ancestor_comparisons += 1
    #         short_count += length < true_length
    #         overlap = np.logical_and(a >= 0, true_ancestor >= 0)
    #         equal_site_count += np.sum(focal_site_time[overlap] == ancestor.time)
    #         younger_site_count += np.sum(focal_site_time[overlap] < ancestor.time)
    #         older_site_count += np.sum(focal_site_time[overlap] > ancestor.time)

    #         mismatch = np.logical_and(overlap, a != true_ancestor)
    #         overlap_count += np.sum(overlap)
    #         mismatch_count += np.sum(mismatch)
    #         overlaps.append(np.sum(overlap))
    #         mismatches.append(np.sum(mismatch))
    #         times.append(ancestor.time)
    #         for mismatch_site in np.where(mismatch)[0]:
    #             assert a[mismatch_site] != true_ancestor[mismatch_site]
    #             mismatch_site_time = focal_site_time[mismatch_site]
    #             index = 0
    #             if mismatch_site_time == ancestor.time:
    #                 index = 1
    #             elif mismatch_site_time < ancestor.time:
    #                 index = 2
    #             # print("\t", mismatch_site, mismatch_site_time, index)
    #             mismatch_type_count[index, a[mismatch_site]] += 1
    #             # print("\tmismatch at ", mismatch_site, a[mismatch_site], true_ancestor[mismatch_site])
    # print("Total mismatch rate = ", mismatch_count / overlap_count)
    # print("Older  site comparisons =", older_site_count)
    # print("Equal  site comparisons =", equal_site_count)
    # print("Younger site comparison =", younger_site_count)
    # print("Short ancestor fraction =", short_count / ancestor_comparisons)
    # print("False 0 at older site   =", mismatch_type_count[0][0], "\t",
    #         mismatch_type_count[0][0] / older_site_count)
    # print("False 1 at older site   =", mismatch_type_count[0][1], "\t",
    #         mismatch_type_count[0][1] / older_site_count)
    # print("False 0 at equal site   =", mismatch_type_count[1][0], "\t",
    #         mismatch_type_count[1][0] / equal_site_count)
    # print("False 1 at equal site   =", mismatch_type_count[1][1], "\t",
    #         mismatch_type_count[1][1] / equal_site_count)
    # print("False 0 at younger site =", mismatch_type_count[2][0])
    # print("False 1 at younger site =", mismatch_type_count[2][1])
    # # print(mismatch_type_count)


    # print(ancestor_data)
    # ancestors_ts = tsinfer.match_ancestors(sample_data, ancestor_data, engine=engine)
    # output_ts = tsinfer.match_samples(subset_samples, ancestors_ts, engine=engine)

    # output_ts = tsinfer.match_samples(sample_data, ancestors_ts, engine=engine)
# #     # dump_provenance(output_ts)
    # G1 = ts.genotype_matrix()
    # G2 = output_ts.genotype_matrix()
    # assert np.array_equal(G1, G2)


def dump_provenance(ts):
    print("dump provenance")
    for p in ts.provenances():
        print("-" * 50)
        print(p.timestamp)
        pprint.pprint(json.loads(p.record))


def build_profile_inputs(n, num_megabases):
    L = num_megabases * 10**6
    input_file = "tmp__NOBACKUP__/profile-n={}-m={}.input.trees".format(
            n, num_megabases)
    if os.path.exists(input_file):
        ts = msprime.load(input_file)
    else:
        ts = msprime.simulate(
            n, length=L, Ne=10**4, recombination_rate=1e-8, mutation_rate=1e-8,
            random_seed=10)
        print("Ran simulation: n = ", n, " num_sites = ", ts.num_sites,
                "num_trees =", ts.num_trees)
        ts.dump(input_file)
    filename = "tmp__NOBACKUP__/profile-n={}-m={}.samples".format(n, num_megabases)
    if os.path.exists(filename):
        os.unlink(filename)
    # daiquiri.setup(level="DEBUG")
    with tsinfer.SampleData(
            sequence_length=ts.sequence_length, path=filename,
            num_flush_threads=4) as sample_data:
        # progress_monitor = tqdm.tqdm(total=ts.num_samples)
        # for j in range(ts.num_samples):
        #     sample_data.add_sample(metadata={"name": "sample_{}".format(j)})
        #     progress_monitor.update()
        # progress_monitor.close()
        progress_monitor = tqdm.tqdm(total=ts.num_sites)
        for variant in ts.variants():
            sample_data.add_site(variant.site.position, variant.genotypes)
            progress_monitor.update()
        progress_monitor.close()

    print(sample_data)

#     filename = "tmp__NOBACKUP__/profile-n={}_m={}.ancestors".format(n, num_megabases)
#     if os.path.exists(filename):
#         os.unlink(filename)
#     ancestor_data = tsinfer.AncestorData.initialise(sample_data, filename=filename)
#     tsinfer.build_ancestors(sample_data, ancestor_data, progress=True)
#     ancestor_data.finalise()

def copy_1kg():
    source = "tmp__NOBACKUP__/1kg_chr22.samples"
    sample_data = tsinfer.SampleData.load(source)
    copy = sample_data.copy("tmp__NOBACKUP__/1kg_chr22_copy.samples")
    copy.finalise()
    print(sample_data)
    print("copy = ")
    print(copy)

def tutorial_samples():
    import tqdm
    import msprime
    import tsinfer

    ts = msprime.simulate(
        sample_size=10000, Ne=10**4, recombination_rate=1e-8,
        mutation_rate=1e-8, length=10*10**6, random_seed=42)
    ts.dump("tmp__NOBACKUP__/simulation-source.trees")
    print("simulation done:", ts.num_trees, "trees and", ts.num_sites,  "sites")

    progress = tqdm.tqdm(total=ts.num_sites)
    with tsinfer.SampleData(
            path="tmp__NOBACKUP__/simulation.samples",
            sequence_length=ts.sequence_length,
            num_flush_threads=2) as sample_data:
        for var in ts.variants():
            sample_data.add_site(var.site.position, var.genotypes, var.alleles)
            progress.update()
    progress.close()


def subset_sites(ts, position):
    """
    Return a copy of the specified tree sequence with sites reduced to those
    with positions in the specified list.
    """
    tables = ts.dump_tables()
    lookup = frozenset(position)
    tables.sites.clear()
    tables.mutations.clear()
    for site in ts.sites():
        if site.position in lookup:
            site_id = tables.sites.add_row(
                site.position, ancestral_state=site.ancestral_state,
                metadata=site.metadata)
            for mutation in site.mutations:
                tables.mutations.add_row(
                    site_id, node=mutation.node, parent=mutation.parent,
                    derived_state=mutation.derived_state,
                    metadata=mutation.metadata)
    return tables.tree_sequence()

def minimise(ts):
    tables = ts.dump_tables()

    out_map = {}
    in_map = {}
    first_site = 0
    for (_, edges_out, edges_in), tree in zip(ts.edge_diffs(), ts.trees()):
        for edge in edges_out:
            out_map[edge.child] = edge
        for edge in edges_in:
            in_map[edge.child] = edge
        if tree.num_sites > 0:
            sites = list(tree.sites())
            if first_site:
                x = 0
                first_site = False
            else:
                x = sites[0].position
            print("X = ", x)
            for edge in out_map.values():
                print("FLUSH", edge)
            for edge in in_map.values():
                print("INSER", edge)

            # # Flush the edge buffer.
            # for left, parent, child in edge_buffer:
            #     tables.edges.add_row(left, x, parent, child)
            # # Add edges for each node in the tree.
            # edge_buffer.clear()
            # for root in tree.roots:
            #     for u in tree.nodes(root):
            #         if u != root:
            #             edge_buffer.append((x, tree.parent(u), u))

    # position = np.hstack([[0], tables.sites.position, [ts.sequence_length]])
    # position = tables.sites.position
    # edges = []
    # print(position)
    # tables.edges.clear()
    # for edge in ts.edges():
    #     left = np.searchsorted(position, edge.left)
    #     right = np.searchsorted(position, edge.right)

    #     print(edge, left, right)
    #     # if right - left > 1:
    #         # print("KEEP:", edge, left, right)
    #         # tables.edges.add_row(
    #         #     position[left], position[right], edge.parent, edge.child)
    #         # print("added", tables.edges[-1])
    #     # else:
    #         # print("SKIP:", edge, left, right)

    # ts = tables.tree_sequence()
    # for tree in ts.trees():
    #     print("TREE:", tree.interval)
    #     print(tree.draw(format="unicode"))





def minimise_dev():
    ts = msprime.simulate(5, mutation_rate=1, recombination_rate=2, random_seed=3)
    # ts = msprime.load(sys.argv[1])

    position = ts.tables.sites.position[::2]
    subset_ts = subset_sites(ts, position)
    print("Got subset")

    ts_new = tsinfer.minimise(subset_ts)
    for tree in ts_new.trees():
        print("TREE:", tree.interval)
        print(tree.draw(format="unicode"))
    # print(ts_new.tables)
    print("done")
    other = minimise(subset_ts)


def run_build():

    sample_data = tsinfer.load(sys.argv[1])
    ad = tsinfer.generate_ancestors(sample_data)
    print(ad)


if __name__ == "__main__":

    # run_build()

    np.set_printoptions(linewidth=20000)
    np.set_printoptions(threshold=20000000)

    # tutorial_samples()

    # build_profile_inputs(10, 10)
    # build_profile_inputs(100, 10)
    # build_profile_inputs(1000, 100)
    # build_profile_inputs(10**4, 100)
    # build_profile_inputs(10**5, 100)

    # for j in range(1, 100):
    #     tsinfer_dev(15, 0.5, seed=j, num_threads=0, engine="P", recombination_rate=1e-8)
    # copy_1kg()
    tsinfer_dev(50, 2.5, seed=4, num_threads=0, engine="P", recombination_rate=1e-8,
            error_rate=0.01)

    # minimise_dev()

#     for seed in range(1, 10000):
#         print(seed)
#         # tsinfer_dev(40, 2.5, seed=seed, num_threads=1, genotype_quality=1e-3, engine="C")
